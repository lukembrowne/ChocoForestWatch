services:
  db:
    image: ghcr.io/stac-utils/pgstac:v${PGSTAC_VERSION-0.9.2}
    container_name: stac-db
    env_file:
      - ./.env
    volumes:
      - ./.pgdata:/var/lib/postgresql/data
    environment:
      - POSTGRES_USER=cfwuser
      - POSTGRES_PASS=1234
      - POSTGRES_DB=cfwdb
      - PGUSER=cfwuser
      - PGDATABASE=cfwdb
    ports:
      - "5432:5432"
    command: postgres -N 500
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U cfwuser -d cfwdb"]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 10s
    networks:
      - app-network

  redis:
    image: redis:latest
    ports:
      - "6379:6379"
    healthcheck:
      test: ["CMD", "redis-cli", "ping"]
      interval: 5s
      timeout: 3s
      retries: 5
    networks:
      - app-network
    environment:
      - TZ=America/New_York
  backend:
    build: 
      context: .
      dockerfile: backend/djangocfw/Dockerfile
    volumes:
      - ./backend/djangocfw:/app
      - ./ml_pipeline:/app/ml_pipeline
      
    ports:
      - "8000:8000"
      - "5678:5678"
      - "3000:3000"
    env_file:
      - ./.env
    image: cfw-backend:latest
    command: python -m debugpy --listen 0.0.0.0:5678 --wait-for-client manage.py runserver 0.0.0.0:8000
    depends_on:
      db:
        condition: service_healthy
      redis:
        condition: service_healthy
    restart: on-failure:3
    networks:
      - app-network
    environment:
      - TZ=America/New_York

  frontend:
    build: 
      context: ./frontend
      args:
        DJANGO_DEBUG: "True"  # Default to production
      target: "development"       # Use 'development' or 'production'
    volumes:  
      - ./frontend:/app
      - /app/node_modules
    ports:
      - "9000:9000"
    env_file:
      - ./.env
    depends_on:
      - backend
    networks:
      - app-network
    environment:
      - TZ=America/New_York

  tiler:
    container_name: tiler-pgstac
    # At the time of writing, rasterio and psycopg wheels are not available for arm64 arch
    # so we force the image to be built with linux/amd64
    platform: linux/amd64
    build:
      context: ./titiler-pgstac
      dockerfile: Dockerfile
    ports:
      - "8081:8081"
    command: ["gunicorn", "-k", "uvicorn.workers.UvicornWorker", "titiler.pgstac.main:app", "--bind", "0.0.0.0:8081", "--workers", "1"]
    environment:
      # Postgres connection
      - POSTGRES_USER=cfwuser
      - POSTGRES_PASS=1234
      - POSTGRES_DBNAME=cfwdb
      - POSTGRES_HOST=db
      - POSTGRES_PORT=5432
      # PG connection
      - DB_MIN_CONN_SIZE=1
      - DB_MAX_CONN_SIZE=1
      # - DB_MAX_QUERIES=10
      # - DB_MAX_IDLE=10
      # GDAL Config
      # This option controls the default GDAL raster block cache size.
      # If its value is small (less than 100000), it is assumed to be measured in megabytes, otherwise in bytes.
      - GDAL_CACHEMAX=200
      - GDAL_DISABLE_READDIR_ON_OPEN=EMPTY_DIR
      - GDAL_INGESTED_BYTES_AT_OPEN=32768
      - GDAL_HTTP_MERGE_CONSECUTIVE_RANGES=YES
      - GDAL_HTTP_MULTIPLEX=YES
      - GDAL_HTTP_VERSION=2
      # The file can be cached in RAM by setting the configuration option VSI_CACHE to TRUE.
      # The cache size defaults to 25 MB, but can be modified by setting the configuration option VSI_CACHE_SIZE (in bytes).
      # Content in that cache is discarded when the file handle is closed.
      - VSI_CACHE=TRUE
      - VSI_CACHE_SIZE=536870912
      # In addition, a global least-recently-used cache of 16 MB shared among all downloaded content is enabled by default,
      # and content in it may be reused after a file handle has been closed and reopen,
      # during the life-time of the process or until VSICurlClearCache() is called.
      # Starting with GDAL 2.3, the size of this global LRU cache can be modified by
      # setting the configuration option CPL_VSIL_CURL_CACHE_SIZE (in bytes).
      - CPL_VSIL_CURL_CACHE_SIZE=200000000
      # TiTiler Config
      - MOSAIC_CONCURRENCY=5
      # AWS S3 endpoint config
      - AWS_ACCESS_KEY_ID=${AWS_ACCESS_KEY_ID}
      - AWS_SECRET_ACCESS_KEY=${AWS_SECRET_ACCESS_KEY}
      - AWS_S3_ENDPOINT=nyc3.digitaloceanspaces.com
      - AWS_REGION=nyc3
    volumes:
      - ./benchmark:/tmp/benchmark
      - ./titiler-pgstac/scripts:/app/scripts
    depends_on:
      - db
    networks:
      - app-network

  tiler-uvicorn:
    container_name: tiler-pgstac-uvicorn
    extends:
      service: tiler
    ports:
      - "8083:8083"
    command: ["uvicorn", "titiler.pgstac.main:app", "--host", "0.0.0.0", "--port", "8083", "--workers", "1"]
    networks:
          - app-network

  nginx:
    image: nginx
    ports:
      - 8080:80
    volumes:
      - ./nginx.conf:/etc/nginx/nginx.conf
    depends_on:
      - app-nginx
    command: [ "nginx-debug", "-g", "daemon off;" ]
    networks:
      - app-network

  app-nginx:
    extends:
      service: tiler
    command: bash -c "uvicorn titiler.pgstac.main:app --host 0.0.0.0 --port 8082 --proxy-headers --forwarded-allow-ips='*' --root-path=/api/v1/titiler"

networks:
  app-network:
    driver: bridge

volumes:
  postgres_data:
  planet_data: