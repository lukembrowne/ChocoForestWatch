services:
  db:
    image: ghcr.io/stac-utils/pgstac:v0.9.2
    container_name: stac-db
    env_file:
      - ./.env
    volumes:
      - ./.pgdata:/var/lib/postgresql/data
    environment:
      - POSTGRES_USER=${POSTGRES_USER}
      - POSTGRES_PASS=${POSTGRES_PASSWORD}
      - POSTGRES_DB=${POSTGRES_DB}
      - PGUSER=${POSTGRES_USER}
      - PGDATABASE=${POSTGRES_DB}
    ports:
      - "5432:5432"
    command: postgres -N 500
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U ${POSTGRES_USER} -d ${POSTGRES_DB}"]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 10s
    networks:
      - app-network

  backend:
    build: 
      context: .
      dockerfile: backend/djangocfw/Dockerfile
    volumes:
      - ./backend/djangocfw:/app
      - ./ml_pipeline:/app/ml_pipeline
    ports:
      - "8000:8000"
      - "5678:5678"
      - "3000:3000"
    env_file:
      - ./.env
    image: cfw-backend:latest
    command: gunicorn djangocfw.wsgi:application --bind 0.0.0.0:8000 --workers 3
    depends_on:
      db:
        condition: service_healthy
    restart: always
    networks:
      - app-network
    environment:
      - TZ=America/New_York
      - DJANGO_DEBUG=False

  frontend:
    build:
      context: ./frontend
      args:
        VITE_API_URL: ${VITE_API_URL}
        VITE_SENTRY_DSN: ${VITE_SENTRY_DSN}
        DJANGO_DEBUG: "False"  # Default to production
        VITE_TITILER_URL: ${VITE_TITILER_URL}
        VITE_DEFAULT_PUBLIC_PROJECT_ID: ${VITE_DEFAULT_PUBLIC_PROJECT_ID}
      target: "production"       # Use 'development' or 'production'
    ports:
      - "9000:9000"
    volumes:  
      - ./frontend:/app
      - /app/node_modules
    env_file:
      - ./.env
    restart: always
    networks:
      - app-network
    environment:
      - TZ=America/New_York

  x-tiler-base: &tiler-base
    platform: linux/amd64
    build:
      context: ./titiler-pgstac
      dockerfile: Dockerfile
    environment:
      # Postgres connection
      - POSTGRES_USER=${POSTGRES_USER}
      - POSTGRES_PASS=${POSTGRES_PASSWORD}
      - POSTGRES_DBNAME=${POSTGRES_DB}
      - POSTGRES_HOST=${DB_HOST}
      - POSTGRES_PORT=${DB_PORT}
      # PG connection
      - DB_MIN_CONN_SIZE=1
      - DB_MAX_CONN_SIZE=8
      # - DB_MAX_QUERIES=10
      # - DB_MAX_IDLE=10
      # GDAL Config
      # This option controls the default GDAL raster block cache size.
      # If its value is small (less than 100000), it is assumed to be measured in megabytes, otherwise in bytes.
      - GDAL_CACHEMAX=512
      - GDAL_DISABLE_READDIR_ON_OPEN=EMPTY_DIR
      - GDAL_INGESTED_BYTES_AT_OPEN=32768
      - GDAL_HTTP_MERGE_CONSECUTIVE_RANGES=YES
      - GDAL_HTTP_MULTIPLEX=YES
      - GDAL_HTTP_VERSION=2
      # The file can be cached in RAM by setting the configuration option VSI_CACHE to TRUE.
      # The cache size defaults to 25 MB, but can be modified by setting the configuration option VSI_CACHE_SIZE (in bytes).
      # Content in that cache is discarded when the file handle is closed.
      - VSI_CACHE=TRUE
      - VSI_CACHE_SIZE=536870912
      # In addition, a global least-recently-used cache of 16 MB shared among all downloaded content is enabled by default,
      # and content in it may be reused after a file handle has been closed and reopen,
      # during the life-time of the process or until VSICurlClearCache() is called.
      # Starting with GDAL 2.3, the size of this global LRU cache can be modified by
      # setting the configuration option CPL_VSIL_CURL_CACHE_SIZE (in bytes).
      - CPL_VSIL_CURL_CACHE_SIZE=268435456
      - WEB_CONCURRENCY=4
      # TiTiler Config
      - MOSAIC_CONCURRENCY=5
      # AWS S3 endpoint config
      - AWS_ACCESS_KEY_ID=${AWS_ACCESS_KEY_ID}
      - AWS_SECRET_ACCESS_KEY=${AWS_SECRET_ACCESS_KEY}
      - AWS_S3_ENDPOINT=nyc3.digitaloceanspaces.com
      - AWS_REGION=nyc3
    volumes:
      - ./benchmark:/tmp/benchmark
      - ./titiler-pgstac/scripts:/app/scripts
    depends_on:
      - db
    networks:
      - app-network

  tiler:
    <<: *tiler-base
    container_name: tiler-pgstac
    # At the time of writing, rasterio and psycopg wheels are not available for arm64 arch
    # so we force the image to be built with linux/amd64
    # platform: linux/amd64
    # build:
    #   context: ./titiler-pgstac
    #   dockerfile: Dockerfile
    ports:
      - "8081:8081"
    command: ["gunicorn", "-k", "uvicorn.workers.UvicornWorker", "titiler.pgstac.main:app", "--bind", "0.0.0.0:8081", "--workers", "1"]


  tiler-uvicorn:
    <<: *tiler-base
    container_name: tiler-pgstac-uvicorn
    ports:
      - "8083:8083"
    command: ["uvicorn", "titiler.pgstac.main:app", "--host", "0.0.0.0", "--port", "8083", "--workers", "1"]
    networks:
          - app-network

  nginx:
    image: nginx
    ports:
      - 8080:80
   # volumes:
   #   - ./nginx.conf:/etc/nginx/nginx.conf
    depends_on:
      - app-nginx
    networks:
      - app-network

  app-nginx:
    <<: *tiler-base
    container_name: tiler-pgstac-uvicorn-app
    ports: 
      - "8082:8082"
    command: bash -c "uvicorn titiler.pgstac.main:app --host 0.0.0.0 --port 8082 --proxy-headers --forwarded-allow-ips='*' --root-path=/api/v1/titiler"

networks:
  app-network:
    driver: bridge

volumes:
  postgres_data:
  planet_data:
